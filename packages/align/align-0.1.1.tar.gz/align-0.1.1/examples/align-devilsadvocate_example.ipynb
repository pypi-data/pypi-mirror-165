{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALIGN Tutorial Notebook: DEVIL'S ADVOCATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to **ALIGN**, \n",
    "a tool for quantifying multi-level linguistic similarity \n",
    "between speakers, using the \"Devil's Advocate\" transcript data reported in Duran, Paxton, and Fusaroli: \"ALIGN: Analyzing Linguistic Interactions with Generalizable techNiques - a Python Library\".  This method was also introduced in Duran, Paxton, and Fusaroli (2019), which can be accessed here for reference: https://osf.io/kx8ur/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Devil's Advocate (\"DA\") study examines interpersonal linguistic alignment between dyads across two conversations where participants either agreed or disagreed with each other (as a randomly assigned between-dyads condition) and where one of the conversations involved the truth and the other deception (as a within-subjects condition), with order of conversations counterbalanced across dyads. \n",
    "\n",
    "**Transcript Data:**\n",
    "\n",
    "The complete de-identified dataset of raw conversational transcripts is hosted on a secure protected access data repository provided by the Inter-university Consortium for Political and Social Research (ICPSR). These transcripts need to be downloaded to use this tutorial. Please click on the link to the ICPSR repository to access: http://dx.doi.org/10.3886/ICPSR37124.v1. \n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "To replicate the results reported in Duran, Paxton, and Fusaroli (2019), or for an example of R code used to analzye the ALIGN output for this dataset, please visit the OSF repository for this project: https://osf.io/3TGUF/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [Highest-level functions](#Highest-level-functions)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [Specify ALIGN path settings](#Specify-ALIGN-path-settings)\n",
    "* [Phase 1: Prepare transcripts](#Phase-1:-Prepare-transcripts)\n",
    "    * [Preparation settings](#Preparation-settings)\n",
    "    * [Run preparation phase](#Run-preparation-phase)\n",
    "* [Phase 2: Calculate alignment](#Phase-2:-Calculate-alignment)\n",
    "    * [For real data: Alignment calculation settings](#For-real-data:-Alignment-calculation-settings)\n",
    "    * [For real data: Run alignment calculation](#For-real-data:-Run-alignment-calculation)\n",
    "    * [For surrogate data: Alignment calculation settings](#For-surrogate-data:-Alignment-calculation-settings)\n",
    "    * [For surrogate data: Run alignment calculation](#For-surrogate-data:-Run-alignment-calculation)\n",
    "* [ALIGN output overview](#ALIGN-output-overview)\n",
    "    * [Speed calculations](#Speed-calculations)\n",
    "    * [Printouts!](#Printouts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The transcript data used for this analysis adheres to the following requirements:**\n",
    "\n",
    "* Each input text file contains a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row corresponds to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "            * **NOTE: For the DA dataset, the label with a value of 0 indicates speaker did not receive any special assignment at the start of the experiment, a value of 1 indicates the speaker has been assigned the role of deceiver (i.e., “devil’s advocate) at the start of the experiment.**\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each conversation text file must be regularly formatted, including a prefix for dyad and a prefix for conversation prior to the identifier for each that are separated by a unique character. By default, ALIGN looks for patterns that follow this convention: `dyad1-condA.txt`\n",
    "    * However, users may choose to include any label for dyad or condition so long as the two labels are distinct from one another and are not subsets of any possible dyad or condition labels. Users may also use any character as a separator so long as it does not occur anywhere else in the filename.\n",
    "    * The chosen file format **must** be used when saving **all** files for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: For the DA dataset, each conversation text file is saved in the format of: dyad_condX-Y-Z (e.g., dyad11_cond1-0-2).**\n",
    "\n",
    "Such that for X, Y, and Z condition codes:\n",
    "\n",
    "* X = Indicates whether the conversation involved dyads who agreed or disagreed with each other: value of 1 indicates a disagreement conversation, value of 2 indicates an agreement conversation (e.g., “cond1”)\n",
    "* Y = Indicates whether the conversation involved deception: value of 0 indicates truth, value of 1 indicates deception.\n",
    "* Z = Indicates conversation order. Given each dyad had two conversations: value of 2 indicates the conversation occurred first, value of 3 indicates the conversation occurred last.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest-level functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given appropriately prepared transcript files, ALIGN can be run in 3 high-level functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`prepare_transcripts`**: Pre-process each standardized \n",
    "conversation, checking it conforms to the requirements. \n",
    "Each utterance is tokenized and lemmatized and has \n",
    "POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`calculate_alignment`**: Generates turn-level and \n",
    "conversation-level alignment scores (lexical, \n",
    "conceptual, and syntactic) across a range of \n",
    "*n*-gram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`calculate_baseline_alignment`**: Generate a surrogate corpus\n",
    "and run alignment analysis (using identical specifications \n",
    "from `calculate_alignment`) on it to produce a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ALIGN if you have not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages we'll need to run ALIGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import align, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `time` so that we can get a sense of how\n",
    "long the ALIGN pipeline takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `warnings` to flag us if required files aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install additional NTLK packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some addition `nltk` packages for `align` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify ALIGN path settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALIGN will need to know where the raw transcripts are stored, where to store the processed data, and where to read in any additional files needed for optional ALIGN parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, specify a base path that will serve as our jumping-off point for our saved data. All of the shipped data will be called from the package directory but the DA transcripts will need to be added manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`BASE_PATH`**: Containing directory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DA_EXAMPLE`**: Subdirectories for output and other\n",
    "files for this tutorial. (We'll create a default directory\n",
    "if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_EXAMPLE = os.path.join(BASE_PATH,\n",
    "                              'DA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DA_EXAMPLE):\n",
    "    os.makedirs(DA_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TRANSCRIPTS`**: Transcript text files must be first downloaded from the ICPSR repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set variable for folder name (as string) for relative location of folder into which the downloaded transcript files need to be manually added. (We'll create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "               'DA-transcripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TRANSCRIPTS):\n",
    "    os.makedirs(TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir(TRANSCRIPTS) :\n",
    "    warnings.warn('DA text files not found at the specified '\n",
    "                  'location. Please download from '\n",
    "                  'http://dx.doi.org/10.3886/ICPSR37124.v1 '\n",
    "                  'and add to directory.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PREPPED_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which \n",
    "prepared transcript files will be saved. (We'll create\n",
    "a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPPED_TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "                                   'DA-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PREPPED_TRANSCRIPTS):\n",
    "    os.makedirs(PREPPED_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ANALYSIS_READY`**: Set variable for folder name \n",
    "(as string) for relative location of folder into \n",
    "which analysis-ready dataframe files will be saved.\n",
    "(We'll create a default directory if one doesn't\n",
    "already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_READY = os.path.join(DA_EXAMPLE,\n",
    "                              'DA-analysis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ANALYSIS_READY):\n",
    "    os.makedirs(ANALYSIS_READY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SURROGATE_TRANSCRIPTS`**: Set variable for folder name \n",
    "(as string) for relative location of folder into which all\n",
    "prepared surrogate transcript files will be saved. (We'll\n",
    "create a default directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURROGATE_TRANSCRIPTS = os.path.join(DA_EXAMPLE,\n",
    "                                     'DA-surrogate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SURROGATE_TRANSCRIPTS):\n",
    "    os.makedirs(SURROGATE_TRANSCRIPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths for optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`OPTIONAL_PATHS`**: If using Stanford POS tagger or\n",
    "pretrained vectors, the path to these files. If these\n",
    "files are provided in other locations, be sure to\n",
    "change the file paths for them. (We'll create a default\n",
    "directory if one doesn't already exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIONAL_PATHS = os.path.join(DA_EXAMPLE,\n",
    "                             'optional_directories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OPTIONAL_PATHS):\n",
    "    os.makedirs(OPTIONAL_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stanford POS tagger **will not be used** by \n",
    "default in this example. However, you may use them\n",
    "by uncommenting and providing the requested file \n",
    "paths in the cells in this section and then changing \n",
    "the relevant parameters in the ALIGN calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, we could use the Standford part-of-speech \n",
    "tagger along with the Penn part-of-speech tagger\n",
    "(which is always used in ALIGN). To do so, the files\n",
    "will need to be downloaded separately: \n",
    "https://nlp.stanford.edu/software/tagger.shtml#Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`STANFORD_POS_PATH`**: If using Stanford POS tagger\n",
    "with the Penn POS tagger, path to Stanford directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANFORD_POS_PATH = os.path.join(OPTIONAL_PATHS,\n",
    "#                                  'stanford-postagger-full-2018-10-16/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(STANFORD_POS_PATH) == False:\n",
    "#     warnings.warn('Stanford POS directory not found at the specified '\n",
    "#                       'location. Please update the file path with '\n",
    "#                       'the folder that can be directly downloaded here: '\n",
    "#                       'https://nlp.stanford.edu/software/stanford-postagger-full-2018-10-16.zip '\n",
    "#                       '- Alternatively, comment out the '\n",
    "#                       '`STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`STANFORD_LANGUAGE`**: If using Stanford tagger,\n",
    "set language model to be used for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANFORD_LANGUAGE = os.path.join('models/english-left3words-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(STANFORD_POS_PATH + STANFORD_LANGUAGE) == False:\n",
    "#     warnings.warn('Stanford tagger language not found at the specified '\n",
    "#                       'location. Please update the file path or comment '\n",
    "#                       'out the `STANFORD_POS_PATH` information.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google News pretrained vectors **will be used**\n",
    "by default in this example. The file is available for\n",
    "download here: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, researchers may choose to read in pretrained\n",
    "`word2vec` vectors rather than creating a semantic space\n",
    "from the corpus provided. This may be especially useful \n",
    "for small corpora (i.e., fewer than 30k unique words),\n",
    "although the choice of semantic space corpus should be\n",
    "made with careful consideration about the nature of the\n",
    "linguistic context (for further discussion, see Duran, \n",
    "Paxton, & Fusaroli, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PRETRAINED_INPUT_FILE`**: If using pretrained vectors, path\n",
    "to pretrained vector files. You may choose to download the file\n",
    "directly to this path or change the path to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_INPUT_FILE = os.path.join(OPTIONAL_PATHS,\n",
    "                            'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(PRETRAINED_INPUT_FILE) == False:\n",
    "    warnings.warn('Google News vector not found at the specified '\n",
    "                      'location. Please update the file path with '\n",
    "                      'the .bin file that can be accessed here: '\n",
    "                      'https://code.google.com/archive/p/word2vec/ '\n",
    "                      '- Alternatively, comment out the `PRETRAINED_INPUT_FILE` information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Phase 1: Prepare transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Phase 1, we take our raw transcripts and get them ready\n",
    "for later ALIGN analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`prepare_transcripts()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(align.prepare_transcripts.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this demonstration, we'll keep everything as\n",
    "defaults. Among other parameters, this means that:\n",
    "* any turns fewer than 2 words will be removed from the corpus\n",
    " (`minwords=2`),\n",
    "* we'll be using regex to strip out any filler words\n",
    " (e.g., \"uh,\" \"um,\" \"huh\"; `use_filler_list=None`),\n",
    "* if you like, you can ignore the regex option and supply additional filler words as `use_filler_list=[\"string1\", \"string2\"]`\n",
    "* moreover, if you like, you can include regex and supply your own filler words, but be sure to set `filler_regex_and_list=True`  \n",
    "* we'll be using the Project Gutenberg corpus to create our \n",
    " spell-checker algorithm (`training_dictionary=None`),\n",
    "* we'll rely only on the Penn POS tagger because the Stanford tagger is quite slow \n",
    " (`add_stanford_tags=False`), and\n",
    "* our data will be saved both as individual conversation files\n",
    " and as a master dataframe of all conversation outputs\n",
    " (`save_concatenated_dataframe=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run preparation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare our transcripts by reading in individual `.txt`\n",
    "files for each conversation, clean up undesired text and turns,\n",
    "spell-check, tokenize, lemmatize, and add POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = align.prepare_transcripts(\n",
    "                        input_files=TRANSCRIPTS,\n",
    "                        output_file_directory=PREPPED_TRANSCRIPTS,\n",
    "                        run_spell_check=True,\n",
    "                        minwords=2,\n",
    "                        use_filler_list=None,\n",
    "                        filler_regex_and_list=False,\n",
    "                        training_dictionary=None,\n",
    "                        add_stanford_tags=False,\n",
    "                        ### if you want to run the Stanford POS tagger, be sure to uncomment the next two lines\n",
    "                        # stanford_pos_path=STANFORD_POS_PATH,\n",
    "                        # stanford_language_path=STANFORD_LANGUAGE,    \n",
    "                        save_concatenated_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Calculate alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For real data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that we can set for the\n",
    "`calculate_alignment()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(align.calculate_alignment.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, we'll keep everything as\n",
    "defaults. Among other parameters, this means that we'll:\n",
    "* use only unigrams and bigrams for our *n*-grams\n",
    " (`maxngram=2`),\n",
    "* use pretrained vectors instead of creating our own\n",
    " semantic space, since our tutorial corpus is quite\n",
    " small (`use_pretrained_vectors=True` and\n",
    " `pretrained_file_directory=PRETRAINED_INPUT_FILE`),\n",
    "* ignore exact lexical duplicates when calculating\n",
    " syntactic alignment,\n",
    "* we'll rely only on the Penn POS tagger \n",
    " (`add_stanford_tags=False`), and\n",
    "* implement high- and low-frequency cutoffs to clean\n",
    " our transcript data (`high_sd_cutoff=3` and \n",
    " `low_n_cutoff=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we calculate a baseline level of alignment,\n",
    "we need to include the same parameter values for any\n",
    "parameters that are present in both `calculate_alignment()`\n",
    "(this step) and `calculate_baseline_alignment()`\n",
    "(next step). As a result, we'll specify these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set standards to be used for real and surrogate\n",
    "INPUT_FILES = PREPPED_TRANSCRIPTS\n",
    "MAXNGRAM = 2\n",
    "USE_PRETRAINED_VECTORS = True\n",
    "SEMANTIC_MODEL_INPUT_FILE = os.path.join(DA_EXAMPLE,\n",
    "                                         'align_concatenated_dataframe.txt')\n",
    "PRETRAINED_FILE_DRIRECTORY = PRETRAINED_INPUT_FILE\n",
    "ADD_STANFORD_TAGS = False\n",
    "IGNORE_DUPLICATES = True\n",
    "HIGH_SD_CUTOFF = 3\n",
    "LOW_N_CUTOFF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For real data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_real,convo_real] = align.calculate_alignment(\n",
    "                            input_files=INPUT_FILES,\n",
    "                            maxngram=MAXNGRAM,   \n",
    "                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                            pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                            output_file_directory=ANALYSIS_READY,\n",
    "                            add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                            ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                            high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                            low_n_cutoff=LOW_N_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For surrogate data: Alignment calculation settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the surrogate or baseline data, we have many of the same\n",
    "parameters for `calculate_baseline_alignment()` as we do for\n",
    "`calculate_alignment()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align.calculate_baseline_alignment.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, when calculating the baseline, it is **vital** \n",
    "to include the *same* parameter values for any parameters that \n",
    "are included  in both `calculate_alignment()` and \n",
    "`calculate_baseline_alignment()`. As a result, we re-use those\n",
    "values here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate other possible uses for labels by setting \n",
    "`dyad_label = time`, allowing us to compare alignment over \n",
    "time across the same speakers. We also demonstrate how to \n",
    "generate a subset of surrogate pairings rather than all \n",
    "possible pairings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the parameters that we're re-using from\n",
    "the `calculate_alignment()` values (see above), we'll \n",
    "keep most parameters at their defaults by:\n",
    "* preserving the turn order when creating surrogate\n",
    " pairs (`keep_original_turn_order=True`),\n",
    "* specifying condition with `cond` prefix \n",
    " (`condition_label='cond'`), and\n",
    "* using a hyphen to separate the condition and\n",
    " dyad identifiers (`id_separator='\\-'`).\n",
    " \n",
    "However, we will also change some of these defaults,\n",
    "including:\n",
    "* generating only a subset of surrogate data equal\n",
    " to the size of the real data (`all_surrogates=False`)\n",
    " and\n",
    "* specifying that we'll be shuffling the baseline data\n",
    " by time instead of by dyad (`dyad_label='time'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For surrogate data: Run alignment calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = align.calculate_baseline_alignment(\n",
    "                                    input_files=INPUT_FILES, \n",
    "                                    maxngram=MAXNGRAM,\n",
    "                                    use_pretrained_vectors=USE_PRETRAINED_VECTORS,\n",
    "                                    pretrained_input_file=PRETRAINED_INPUT_FILE,\n",
    "                                    semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,\n",
    "                                    output_file_directory=ANALYSIS_READY,\n",
    "                                    add_stanford_tags=ADD_STANFORD_TAGS,\n",
    "                                    ignore_duplicates=IGNORE_DUPLICATES,\n",
    "                                    high_sd_cutoff=HIGH_SD_CUTOFF,\n",
    "                                    low_n_cutoff=LOW_N_CUTOFF,\n",
    "                                    surrogate_file_directory=SURROGATE_TRANSCRIPTS,\n",
    "                                    all_surrogates=False,\n",
    "                                    keep_original_turn_order=True,\n",
    "                                    id_separator='\\_',\n",
    "                                    dyad_label='dyad',\n",
    "                                    condition_label='cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALIGN output overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, let's take a look at how long it takes to run each section. Time is given in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase1 - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 2, real data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2real - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 2, surrogate data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All phases:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_phase2surrogate - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Before we go, let's take a look at the output from the real data analyzed at the turn level for each conversation (`turn_real`) and at the conversation level for each dyad (`convo_real`). We'll then look at our surrogate data, analyzed both at the turn level (`turn_surrogate`) and at the conversation level (`convo_surrogate`). In our next step, we would then take these data and plug them into our statistical model of choice. As an example of how this was done for Duran, Paxton, and Fusaroli (2019, *Psychological Methods*, https://doi.org/10.1037/met0000206) please visit: https://osf.io/3TGUF/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('align0.1.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc25a66fa7830f2a29e06244408dbdd28f18a71635d925facf2272c6ec49747e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
