# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/compose.ipynb.

# %% auto 0
__all__ = ['Sequential', 'MultiComponent', 'Pipeline', 'make_pipeline', 'pipeline_factory', 'PandasPipeline', 'ParallelCallback',
           'Parallel', 'MultiModalityCallback', 'MultiModality', 'ColumnSelector', 'Concat', 'ColumnTransformer',
           'Identity', 'make_column_transformer_pipelines', 'make_column_transformer', 'MultiSplitComponent',
           'MultiSplitDict', 'MultiSplitDFColumn', 'ParallelInstancesCallback', 'ParallelInstances',
           'CrossValidatorCallback', 'CrossValidator', 'finalize_result_parallel_models',
           'ParallelModelInstancesCallback', 'ParallelModelInstances', 'WeightedClassifier', 'make_ensembler']

# %% ../../nbs/core/compose.ipynb 3
import abc # comment # comment
import warnings
import sys
from pathlib import Path
import shutil
import copy
import warnings
import sh
from functools import partial

import joblib
import json
from sklearn.utils import Bunch
import pandas as pd
import numpy as np

try:
    from graphviz import *
    imported_graphviz = True
except:
    imported_graphviz = False

from .components import (Component,
                                          PandasComponent,
                                          SamplingComponent,
                                          NoSaverComponent)
from .data_conversion import PandasConverter
from .utils import PandasIO
from ..utils.utils import get_logging_level, set_empty_logger
import dsblocks.config.bt_defaults as dflt
from ..blocks.blocks import SkSplitGenerator
from ..utils.utils import get_calling_modules, add_commit_files

# %% ../../nbs/core/compose.ipynb 6
class MultiComponent (SamplingComponent):
    """
    Component containing a list of components inside.

    The list must contain at least one component.

    See `Pipeline` class.
    """
    def __init__ (self,
                  *components,
                  separate_labels=dflt.separate_labels,
                  warning_if_nick_name_exists=dflt.warning_if_nick_name_exists,
                  propagate=dflt.propagate,
                  path_results=dflt.path_results,
                  path_models=dflt.path_models,
                  root=None,
                  automatic_root=False,
                  unique_names=True,
                  **kwargs):
        """Assigns attributes and calls parent constructor.

        Parameters
        ----------
        separate_labels: bool, optional
            whether or not the fit method receives the labels in a separate `y` vector
            or in the same input `X`, as an additional variable. See description of
            Pipeline class for more details.
        """
        if 'estimator' in kwargs:
            self.logger.warning ('estimator passed as key-word argument in MultiComponent')

        self.warning_if_nick_name_exists = warning_if_nick_name_exists

        if len(components) > 0:
            self.set_components (*components, **kwargs)
        elif not hasattr (self, 'components'):
            self.components = []
        if not hasattr (self, 'finalized_component_list'):
            self.finalized_component_list = False

        # we need to call super().__init__() *after* having creating the `components` field,
        # since the constructor of Component calls a method that is overriden in Pipeline,
        # and this method makes use of the mentioned `components` field
        if root == True: root = self
        elif automatic_root: root = self if root is None else root

        super().__init__ (separate_labels=separate_labels, path_results=path_results, path_models=path_models,
                          root=root, **kwargs)
        if self.root == True: root = self

        self.set_split ('whole')

        self.chain_folders (self.data_io.folder)
        if self.propagate:
            self.set_path_results (self.path_results)
            self.set_path_models (self.path_models)

        self.start_idx = dict (apply = dict (training=0, validation=0, test=0, whole=0),
                               fit = dict (training=0, validation=0, test=0, whole=0))
        self.is_data_source = dict (apply = dict (training=False, validation=False, test=False, whole=False),
                               fit = dict (training=False, validation=False, test=False, whole=False))
        self.all_components_fitted = False
        self.load_all_estimators = False

        self.set_root_info (root, unique_names=unique_names, **kwargs)

    def set_root_info (self, root, unique_names=True, **kwargs):
        if root is not None: self.set_root (root)
        if root is self:
            self.num_names = {}
            self.names = {}
            if unique_names: self.set_unique_names ()
            self.gather_and_save_info (**kwargs)

    def __repr__ (self):
        return f'MultiComponent {self.class_name} (name={self.name})'

    def gather_and_save_info (self, path_results=None, path_session=None, split=None,
                              remove_non_pickable=False, **kwargs):
        self.gather_descendants ()
        self.find_last_result (split=split)
        self.find_last_fitted_model (split=split)
        self.save_object (path_results=path_results, path_session=path_session,
                          remove_non_pickable=remove_non_pickable)
        if self.path_results is not None:
            stack_paths = self.save_call_stack (**kwargs)
            self.add_metadata_to_repo (stack_paths=stack_paths, **kwargs)

    def save_call_stack (self, folder='all', **kwargs):
        call_stack = get_calling_modules (folder=folder, **kwargs)
        call_stack = [dict (zip (('path', 'lineno', 'function', 'context', 'flag'), x[1:])) for x in call_stack]
        config_results = self.path_results / 'config'
        config_results.mkdir (parents=True, exist_ok=True)
        with open (config_results / 'call_stack.json', 'wt') as f: json.dump (call_stack, f, indent=4)
        stack_paths = [x['path'] for x in call_stack]
        return stack_paths

    def add_metadata_to_repo (self, file_paths=None, stack_paths=None, additional_file_paths=None,
                              add_pipeline=False, track_files=False, recursive=True,
                              commit_message=None, **kwargs):
        if self.path_results is None:
            raise ValueError ('self.path_results must be set before calling add_metadata_to_repo')
        git = sh.git.bake ()
        current_hash = str(git ('rev-parse', 'HEAD'))
        config_results = self.path_results / 'config'
        config_results.mkdir (parents=True, exist_ok=True)
        metadata = dict (root_path=str(self.path_results), current_hash=current_hash)
        with open (config_results / 'root_metadata.json', 'wt') as f: json.dump (metadata, f, indent=4)
        if file_paths is None:
            if stack_paths is None: stack_paths = []
            if additional_file_paths is None: additional_file_paths = []
            metadata_paths = [config_results / 'call_stack.json', config_results / 'root_metadata.json']
            file_paths = stack_paths + additional_file_paths + metadata_paths
            if add_pipeline:
                file_paths.append (self.path_results / 'pipeline.pk')
        if recursive:
            path_results_set = {str(self.path_results)}
            other_file_paths = self.write_git_hash_and_root_path (path_results_set, self.path_results,
                    current_hash)
        else:
            other_file_paths = []
        if track_files:
            if commit_message is None: commit_message = f'new metadata files in {self.path_results.name}'
            add_commit_files (file_paths+other_file_paths,
                message=commit_message)
            current_hash = str(git ('rev-parse', 'HEAD'))
            metadata['current_hash'] = current_hash
            with open (config_results / 'next_hash.json', 'wt') as f: json.dump (metadata, f, indent=4)

    def write_git_hash_and_root_path (self, path_results_set, root_path, current_hash):
        file_paths = self.data_io.write_git_hash_and_root_path (path_results_set, root_path, current_hash)
        for component in self.components:
            if isinstance (component, MultiComponent):
                file_paths.extend (component.write_git_hash_and_root_path (path_results_set, root_path,
                    current_hash))
            else:
                file_paths.extend (component.data_io.write_git_hash_and_root_path (path_results_set, root_path,
                    current_hash))
        return file_paths

    def show (self, hierarchy_level=np.inf, horizontal=False):
        comps = [(k, self.cls[k]) if not isinstance(self.cls[k], list) else (k, self.cls[k][0]) for k in self.cls]
        comps = [x[0] for x in comps if x[1].hierarchy_level < hierarchy_level]
        comps = sorted(comps)
        if horizontal:
            print (comps)
        else:
            for k in comps:
                print (k)

    def showh (self, max_level=10000, vline=True):
        char = ['*','=','+', '-', '.']
        def hierarchy (self, level=0, before=None, i=0):
            if vline:
                line = char[level-1]*50 if (level <= len(char) and level > 0) else ''
                space = " "*level
                line = (space + line)[:50]
                print (line)
            suffix = '' if before is None else f'{before}: '
            print (f'{suffix}{self.class_name}')
            if (level < max_level) and isinstance (self, MultiComponent):
                space = " "*(level+1)
                for i, c in enumerate(self.components):
                    new_before = f'{space}{before}.{i}' if before is not None else f'{space}{i}'
                    hierarchy (c, level=level+1, before=new_before, i=i)
        hierarchy (self)

    def register_components (self, *components):
        """
        Registering component in `self.components` list.

        Every time that a new component is set as an attribute of the pipeline,
        this component is added to the list `self.components`. Same
        mechanism as the one used by pytorch's `nn.Module`
        """
        if not hasattr(self, 'components'):
            self.components = []
            self.finalized_component_list = False
        if not self.finalized_component_list:
            self.components += components

    def _add_named_attribute (self, component, nick_name):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        if not self.finalized_component_list:
            if not hasattr(self, component.name):
                super().__setattr__(component.name, component)
            if hasattr(component, 'nick_name') and self.warning_if_nick_name_exists:
                self.logger.warning (f'{component} already has a nick_name: {component.nick_name}')
                warnings.warn (f'{component} already has a nick_name: {component.nick_name}')
            component.nick_name = nick_name

    def __setattr__(self, k, v):
        """
        See register_components
        """
        super().__setattr__(k, v)

        if isinstance(v, Component):
            self.register_components(v)
            self._add_named_attribute (v, k)

    def set_but_not_add_component_attr (self, k, v):
        super().__setattr__(k, v)

    def add_component (self, component):
        if not hasattr(self, 'finalized_component_list'):
            self.finalized_component_list = False
        finalized_component_list = self.finalized_component_list
        self.finalized_component_list = False
        self.register_components(component)
        self._add_named_attribute (component, component.name)
        self.finalized_component_list = finalized_component_list

    def set_components (self, *components, **kwargs):
        self.components = components
        for i, component in enumerate(components):
            component, changed = self.obtain_component (component, **kwargs)
            self._add_named_attribute (component, component.name)
            if changed:
                self.components = list(self.components)
                self.components[i] = component
        self.finalized_component_list = True

    def obtain_component (self, component, **kwargs):
        if isinstance (component, Component):
            return component, False
        elif component.__class__.__name__ == 'function':
            return Component (apply=component, **kwargs), True
        elif isinstance(component, tuple):
            assert (len(component)==2 and isinstance(component[0], str)
                    and component[1].__class__.__name__ == 'function')
            return Component (apply=component[1], class_name=component[0], **kwargs), True
        else:
            return Component (estimator=component, **kwargs), True

    def clear_descendants (self):
        self.cls = Bunch ()
        self.obj = Bunch ()
        self.full_obj = Bunch ()
        self.full_cls = Bunch ()
        for component in self.components:
            if isinstance(component, MultiComponent):
                component.clear_descendants ()

    def gather_descendants (self, root='', nick_name=True):
        self.cls = Bunch ()
        self.obj = Bunch ()
        self.full_obj = Bunch ()
        self.full_cls = Bunch ()

        if hasattr(self, 'nick_name'):
            name = self.nick_name if nick_name else self.name
        else:
            name = self.name
        self.hierarchy_path = f'{root}{name}'
        for component in self.components:
            self._insert_descendant (self.cls, component, component.class_name)
            self._insert_descendant (self.obj, component, component.name)

            name = component.nick_name if nick_name else component.name
            component_hierarchy_path = f'{self.hierarchy_path}.{name}'
            self._insert_descendant (self.full_cls, component_hierarchy_path, component.class_name)
            self._insert_descendant (self.full_obj, component_hierarchy_path, component.name)
            if isinstance(component, MultiComponent):
                component.gather_descendants (root=f'{self.hierarchy_path}.',
                                              nick_name=nick_name)
                for name in component.cls:
                    self._insert_descendant (self.cls, component.cls[name], name)
                    self._insert_descendant (self.full_cls, component.full_cls[name], name)
                for name in component.obj:
                    self._insert_descendant (self.obj, component.obj[name], name)
                    self._insert_descendant (self.full_obj, component.full_obj[name], name)

    def _insert_descendant (self, cmp_dict, component, name):
        if name in cmp_dict:
            if not isinstance(cmp_dict[name], list):
                cmp_dict[name] = [cmp_dict[name]]
            if isinstance(component, list):
                cmp_dict[name].extend(component)
            else:
                cmp_dict[name].append(component)
        else:
            if isinstance(component, list):
                cmp_dict[name] = component.copy()
            else:
                cmp_dict[name] = component

    def gather_times (self, root=True):
        times = self.profiler.retrieve_times ()
        multi_comp_ovh = times.avg.drop(columns='leaf')
        index = multi_comp_ovh.index
        multi_comp_ovh = multi_comp_ovh.reset_index(drop=True)
        multi_comp_ovh_aggregated = multi_comp_ovh.sum(axis=1)
        times['multi_comp_ovh'] = multi_comp_ovh
        times['multi_comp_ovh_aggregated'] = multi_comp_ovh_aggregated
        dfs = [times]
        for component in self.components:
            c_times = component.profiler.retrieve_times ()
            c_multi_comp_ovh = c_times.avg.drop(columns='leaf').reset_index(drop=True)
            c_multi_comp_ovh_aggregated = c_multi_comp_ovh.sum(axis=1)
            times['multi_comp_ovh'] -= c_multi_comp_ovh
            times['multi_comp_ovh_aggregated'] -= c_multi_comp_ovh_aggregated
            if isinstance(component, MultiComponent):
                dfs.extend(component.gather_times (root=False))
            else:
                c_times = component.profiler.retrieve_times (is_leaf=True)
                c_multi_comp_ovh = c_times.avg.drop(columns='leaf')
                c_multi_comp_ovh.loc[:] = None
                c_multi_comp_ovh_aggregated = c_multi_comp_ovh.sum(axis=1)
                c_multi_comp_ovh_aggregated.loc[:] = None
                c_times['multi_comp_ovh'] = c_multi_comp_ovh
                c_times['multi_comp_ovh_aggregated'] = c_multi_comp_ovh_aggregated
                dfs.append(c_times)
        multi_comp_ovh.index = index
        multi_comp_ovh_aggregated.index = index
        if root:
            dfs = self.profiler.combine_times (dfs)
            dfs = self.profiler.analyze_overhead (dfs)
        return dfs

    def construct_diagram (self, split=None, include_url=False, port=4000, project='dsblocks'):
        """
        Construct diagram of the pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        if include_url:
            base_url = f'http://localhost:{port}/{project}'
        else:
            URL = ''

        node_name = 'data'
        output = 'train / test'

        f = Digraph('G', filename='fsm2.svg')
        f.attr('node', shape='circle')

        f.node(node_name)

        f.attr('node', shape='box')
        for component in self.components:
            last_node_name = node_name
            last_output = output
            node_name = component.model_plotter.get_node_name()
            if include_url:
                URL = f'{base_url}/{component.model_plotter.get_module_path()}.html#{node_name}'
            f.node(node_name, URL=URL)
            f.edge(last_node_name, node_name, label=last_output)
            output = component.model_plotter.get_edge_name(split=split)

        last_node_name = node_name
        node_name = 'output'
        f.attr('node', shape='circle')
        f.edge(last_node_name, node_name, label=output)

        return f

    def show_result_statistics (self, split=None):
        """
        Show statistics about results obtained by each component.

        By default, this is shown on test data, although this can change setting
        `split='train'`
        """
        split = self.get_split (split)

        for component in self.components:
            component.show_result_statistics(split=split)

    def show_summary (self, split=None, file=sys.stdout):
        """
        Show list of pipeline components, data flow and dimensionality.

        By default, we use test data to show the number of observations
        in the output of each component. This can be changed passing
        `split='train'`
        """
        split = self.get_split (split)

        node_name = 'data'
        output = 'train / test'
        if isinstance (file, str) or isinstance (file, Path): file = open (file, 'wt')

        for i, component in enumerate(self.components):
            node_name = component.model_plotter.get_node_name()
            output = component.model_plotter.get_edge_name(split=split)
            print (f'{"-"*100}', file=file)
            print (f'{i}: {node_name} => {output}', file=file)


    def get_split (self, split=None):
        if split is None:
            if self.data_io.split is not None:
                split = self.data_io.split
            else:
                split = 'whole'

        return split

    def assert_all_equal (self, path_reference_results, raise_error=False, recursive=True,
                          max_recursion=None, current_recursion=0, verbose=None, **kwargs):
        """Compare results stored in current run against reference results stored in given path."""
        if verbose is not None:
            self.logger.setLevel(get_logging_level (verbose))
        is_equal = True
        non_equal_components = []
        end_recursion = max_recursion is not None and current_recursion >= max_recursion
        components = self.components if not end_recursion else [self]
        for component in components:
            if isinstance(component, MultiComponent) and recursive and not end_recursion:
                this_equal = component.assert_all_equal (path_reference_results,
                                                         raise_error=raise_error,
                                                         recursive=recursive,
                                                         max_recursion=max_recursion,
                                                         current_recursion=current_recursion+1,
                                                         verbose=verbose,
                                                         **kwargs)
            else:
                this_equal = component.assert_equal (path_reference_results,
                                                     raise_error=raise_error,
                                                     verbose=verbose,
                                                     **kwargs)
            if not this_equal:
                non_equal_components.append(component.name)
            is_equal = this_equal and is_equal

        if not is_equal:
            self.logger.warning (f'Results are different in components {non_equal_components}')
        else:
            self.logger.info ('both pipelines give the same results')

        self.logger.setLevel(get_logging_level (self.verbose))

        return is_equal

    def load_estimator (self, skip_from=None):
        for component in self.components[:skip_from]:
            component.load_estimator ()

    def save_result (self, result, split=None, path_results=None, result_file_name=None):
        raise NotImplementedError ()
        self.data_io.save_result (result, split=split, path_results=path_results,
                                  result_file_name=result_file_name)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.save_result (result, split=split, path_results=path_results,
                                       result_file_name=result_file_name)
            else:
                component.data_io.save_result (result, split=split, path_results=path_results,
                                               result_file_name=result_file_name)

    def save_object (self, save_run=True, path_results=None, path_session=None,
                     remove_non_pickable=False):
        if remove_non_pickable:
            pipe = copy.deepcopy (self)
            pipe.remove_non_pickable_fields ()
        else:
            pipe = self
        path_results = pipe.data_io.path_results if path_results is None else Path(path_results)
        if path_results is not None:
            path_results.mkdir (parents=True, exist_ok=True)
            try:
                joblib.dump (pipe, path_results / 'pipeline.pk')
            except Exception as e:
                self.logger.warning (f'could not pickle object: {e}')
        if save_run:
            if path_session is None: path_session = dflt.path_session_folder
            path_session = Path (path_session) / f'last_run/{dflt.session_filename}'
            path_session.parent.mkdir (parents=True, exist_ok=True)
            try:
                joblib.dump (pipe, path_session)
            except Exception as e:
                self.logger.warning (f'could not pickle object: {e}')

    def save_logs (self, save_run=True, path_results=None, path_session=None):
        path_log_file = f'{dflt.path_logger_folder}/{dflt.logger_filename}'
        if Path(path_log_file).exists():
            if path_results is None:  path_results = self.data_io.path_results
            if path_results is not None:
                path_results.mkdir (parents=True, exist_ok=True)
                shutil.copy (path_log_file, path_results)
            if save_run:
                if path_session is None: path_session = dflt.path_session_folder
                path_session_log_file = Path (path_session) / f'last_run/{dflt.logger_filename}'
                path_session_log_file.parent.mkdir (parents=True, exist_ok=True)
                shutil.copy (path_log_file, path_session_log_file)


    def remove_non_pickable_fields (self):
        for component in self.components:
            component.remove_non_pickable_fields ()

    def find_last_result (self, split=None):
        return False

    def find_last_fitted_model (self, split=None):
        return False

    def find_method (self, method):
        if callable(getattr (self, method, None)):
            return getattr (self, method, None)
        elif (isinstance (self, Sequential) or isinstance (self, MultiSplitComponent) or
             isinstance (self, ParallelInstances)):
            if isinstance (self.components[-1], MultiComponent):
                return self.components[-1].find_method (method)
            elif callable(getattr (self.components[-1], method, None)):
                return getattr (self.components[-1], method, None)
        return None

    # *************************
    # setters
    # *************************
    def set_split (self, split):
        super().set_split (split)
        for component in self.components:
            component.set_split (split)

    def set_save_splits (self, save_splits):
        super().set_save_splits (save_splits)
        for component in self.components:
            component.set_save_splits (save_splits)

    def set_load_model (self, load_model):
        super().set_load_model (load_model)
        for component in self.components:
            component.set_load_model (load_model)

    def set_save_model (self, save_model):
        super().set_save_model (save_model)
        for component in self.components:
            component.set_save_model (save_model)

    def set_save_result (self, save_result):
        super().set_save_result (save_result)
        for component in self.components:
            component.set_save_result (save_result)

    def set_load_result (self, load_result):
        super().set_load_result (load_result)
        for component in self.components:
            component.set_load_result (load_result)

    def set_path_results (self, path_results):
        self.data_io.set_path_results (path_results)
        for component in self.components:
            if not component.data_io.stop_propagation:
                if isinstance (component, MultiComponent):
                    component.set_path_results (path_results)
                else:
                    component.data_io.set_path_results (path_results)
    def set_path_models (self, path_models):
        self.data_io.set_path_models (path_models)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_path_models (path_models)
            else:
                component.data_io.set_path_models (path_models)
    def chain_folders (self, folder, root=True):
        if folder == '':
            return
        if root:
            self.data_io.chain_folders ('')
        else:
            self.data_io.chain_folders (folder)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.chain_folders (folder, root=False)
            else:
                component.data_io.chain_folders (folder)

    def set_root (self, root):
        super().__setattr__ ('root', root)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_root (root)
            else:
                component.root = root

    def register_global_name (self, component):
        if component.name in self.root.names:
            if component.name not in self.root.num_names:
                self.root.num_names[component.name] = 0
            self.root.num_names[component.name] += 1
            component.set_name (f'{component.name}_{self.root.num_names[component.name]}')
        self.root.names[component.name] = component

    def set_unique_names (self):
        self.register_global_name (self)
        for component in self.components:
            if isinstance (component, MultiComponent):
                component.set_unique_names ()
            else:
                self.register_global_name (component)

    def set_suffix (self, suffix):
        super().set_suffix (suffix)
        for component in self.components:
            component.set_suffix (suffix)

# %% ../../nbs/core/compose.ipynb 52
class Pipeline (MultiComponent):
    """
    Pipeline composed of a list of components that run sequentially.

    During training, the components of the list are trained one after the other,
    where one component is fed the result of transforming the data with the list
    of components located before in the pipeline.

    The `Pipeline` class is a subclass of `SamplingComponent`, which itself is a
    subclass of `Component`. This provides the functionality of `Component`
    to any implemented pipeline, such as logging the messages, loading / saving the
    results, and convert the data format so that it can work as part of other
    pipelines with potentially other data formats.

    Being a subclass of `SamplingComponent`, the `transform` method
    receives an input data  `X` that contains both data and labels.

    Furthermore, the Pipeline constructor sets `separate_labels=False` by default,
    which means that the `fit` method also receives an input data `X` that contains
    not only data but also labels. This is necessary because some of the components in
    the pipeline might be of class `SamplingComponent`, and such components
    need the input data `X` to contain labels when calling `transform` (and note that
    this method is called when calling `fit` on a pipeline, since we do `fit_transform`
    on all the components except for the last one)
    """
    def __init__ (self, *components, **kwargs):
        """Assigns attributes and calls parent constructor.

        Parameters
        ----------
        separate_labels: bool, optional
            whether or not the fit method receives the labels in a separate `y` vector
            or in the same input `X`, as an additional variable. See description of
            Pipeline class for more details.
        """

        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return f'Sequential {self.class_name} (name={self.name})'

    def _fit (self, *X, **kwargs):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        assert len(self.components) > 1, 'Sequential class needs to have more than one component'
        X = self._fit_apply (*X, last=-1, **kwargs)
        self.components[-1].fit (X, **kwargs)

    def _fit_apply (self, *X, split=None, last=None, **kwargs):
        split = self.data_io.split if split is None else split
        first = self.start_idx['fit'][split]
        if first >= len(self.components):
            return X
        if first > 0:
            self.load_estimator (skip_from=first)

        if first < len(self.components):
            component = self.components[first]
            X = component.fit_apply (*X, sequential_fit_apply=True, split=split, **kwargs)
            first += 1
        for component in self.components[first:last]:
            X = component.fit_apply (X, sequential_fit_apply=True, split=split, **kwargs)
        return X

    def _apply (self, *X, split=None, **kwargs):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        split = self.data_io.split if split is None else split
        first = self.start_idx['apply'][split]

        if first < len(self.components):
            component = self.components[first]
            X = component.apply (*X, split=split, **kwargs)
            first += 1
        for component in self.components[first:]:
            X = component.apply (X, split=split, **kwargs)
        return X

    def find_last_result (self, split=None, func='apply', first=-1):

        idx = None
        for i, component in enumerate(self.components[first::-1]):
            if component.data_io.can_load_result () and component.data_io.exists_result (split=split):
                idx = i
                break
            elif isinstance (component, MultiComponent):
                starting_point = component.find_last_result (split=split)
                if starting_point:
                    idx = i
                    break
        split = self.data_io.split if split is None else split
        if idx is not None:
            first = (len(self.components) + first) if (first < 0) else first
            self.start_idx[func][split] = first - idx
            self.is_data_source[func][split] = True
        else:
            self.start_idx[func][split] = 0
            self.is_data_source[func][split] = False
        return self.is_data_source[func][split]

    def find_last_fitted_model (self, split=None):
        idx = len(self.components)-1
        all_components_fitted = True
        self.load_all_estimators = False
        for i, component in enumerate(self.components):
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    idx = i-1
                    all_components_fitted = False
                    break
            elif (component.is_model and
                  not (component.data_io.can_load_model () and component.data_io.exists_estimator ())):
                    idx = i-1
                    all_components_fitted = False
                    break

        if idx >= 0:
            _ = self.find_last_result (split=split, func='fit', first=idx)
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# Sequential is an alias of Pipeline
Sequential = Pipeline

# %% ../../nbs/core/compose.ipynb 74
def make_pipeline(*components, cls=Pipeline, **kwargs):
    """Create `Pipeline` object of class `cls`, given `components` list."""
    pipeline = cls (**kwargs)
    pipeline.set_components(*components)
    return pipeline

# %% ../../nbs/core/compose.ipynb 78
def pipeline_factory (pipeline_class, **kwargs):
    """Creates a pipeline object given its class `pipeline_class`

    Parameters
    ----------
    pipeline_class : class or str
        Name of the pipeline class used for creating the object.
        This can be either of type string or class.
    """
    if type(pipeline_class) is str:
        Pipeline = eval(pipeline_class)
    elif type(pipeline_class) is type:
        Pipeline = pipeline_class
    else:
        raise ValueError (f'pipeline_class needs to be either string or class, we got {pipeline_class}')

    return Pipeline (**kwargs)

# %% ../../nbs/core/compose.ipynb 82
class PandasPipeline (Pipeline):
    """
    Pipeline that saves results in parquet format, and preserves DataFrame format.

    See `Pipeline` class for an explanation of using `separate_labels=False`
    """
    def __init__ (self,
                  data_converter='PandasConverter',
                  data_io='PandasIO',
                  separate_labels=False,
                  **kwargs):
        super().__init__ (data_converter=data_converter,
                          data_io=data_io,
                          separate_labels=separate_labels,
                          **kwargs)

# %% ../../nbs/core/compose.ipynb 87
class ParallelCallback ():
    def __init__ (self, parallel, select_input_to_fit=None, select_input=None,
                  initialize_result=None, join_result=None, finalize_result=None, **kwargs):
        self.parallel = parallel

        select_input_to_fit = (self.select_input_to_fit if select_input_to_fit is None
                                      else partial(select_input_to_fit, self))
        self.select_input_to_fit = select_input_to_fit
        select_input = (self.select_input if select_input is None else partial(select_input, self))
        self.select_input = select_input
        initialize_result = (self.initialize_result if initialize_result is None
                                      else partial(initialize_result, self))
        self.initialize_result = initialize_result
        join_result = (self.join_result if join_result is None
                                      else partial(join_result, self))
        self.join_result = join_result
        finalize_result = (self.finalize_result if finalize_result is None
                                      else partial(finalize_result, self))
        self.finalize_result = finalize_result

    def __getattr__ (self, k): return getattr (self.parallel, k)

    # *****************************************
    # hook functions for results handling
    # *****************************************
    def initialize_result (self):
        return []

    def select_input (self, components, i, *X):
        return X

    def select_input_to_fit (self, components, i, *X):
        return X

    def join_result (self, Xr, Xi_r, components, i):
        Xr.append (Xi_r)
        return Xr

    def finalize_result (self, Xr, components=None):
        if type(Xr) is list: Xr = tuple(Xr)
        return Xr

    # **********************************************************************************
    # hook functions setting and storing information at the beginning of each method,
    # at each iteration, and at the end of each method
    # **********************************************************************************
    def set_component_info (self, component, i):
        pass
    def store_component_fit_info (self, component, i):
        pass
    def store_component_apply_info (self, component, i):
        pass
    def store_component_fit_apply_info (self, component, i):
        pass
    def store_component_find_last_result_info (self, component, i):
        pass
    def store_component_find_last_fitted_model_info (self, component, i):
        pass
    def store_fit_info (self):
        pass
    def store_fit_apply_info (self):
        pass
    def store_apply_info (self):
        pass

class Parallel (MultiComponent):
    """
    List of components that don't have sequential dependencies.

    As the name suggests, these components could run in parallel,
    if a concurrency mechanism is employed.
    """
    def __init__ (self, *components, callback=None, **kwargs):
        """Assigns attributes and calls parent constructor.
        """
        self.cb = ParallelCallback (self, **kwargs) if callback is None else callback
        self.force_end = False

        super().__init__ (*components, **kwargs)

    def __repr__ (self):
        return f'Parallel {self.class_name} (name={self.name})'

    def _fit (self, *X, **kwargs):
        """
        Fit components of the pipeline, given data X and labels y.

        By default, y will be None, and the labels are part of `X`, as a variable.
        """
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input_to_fit (self.components, i, *X)
            component.fit (*Xi, **kwargs)
            self.cb.store_component_fit_info (component, i)
            if self.force_end:
                self._finish_loop (i)
                break
        self.cb.store_fit_info ()

    def _finish_loop (self, i):
        message = f"finishing fit's loop at iteration {i}"
        #print (message)
        self.logger.info (message)
        self.force_end = False

    def _apply (self, *X, **kwargs):
        """Transform data with components of pipeline, and predict labels with last component.

        In the current implementation, we consider prediction a form of mapping,
        and therefore a special type of transformation."""
        Xr = self.cb.initialize_result ()
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input (self.components, i, *X)
            Xi_r = component.apply (*Xi, **kwargs)
            Xr = self.cb.join_result (Xr, Xi_r, self.components, i)
            self.cb.store_component_apply_info (component, i)

        Xr = self.cb.finalize_result (Xr)
        self.cb.store_apply_info ()

        return Xr

    def _fit_apply (self, *X, **kwargs):
        Xr = self.cb.initialize_result ()
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            Xi = self.cb.select_input_to_fit (self.components, i, *X)
            Xi_r = component.fit_apply (*Xi, **kwargs)
            Xr = self.cb.join_result (Xr, Xi_r, self.components, i)
            self.cb.store_component_fit_apply_info (component, i)
            if self.force_end:
                self._finish_loop (i)
                break

        Xr = self.cb.finalize_result (Xr)
        self.cb.store_fit_apply_info ()

        return Xr

    def find_last_result (self, split=None, func='apply'):
        is_data_source = True
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            if not (component.data_io.can_load_result () and component.data_io.exists_result (split=split)):
                if isinstance (component, MultiComponent):
                    is_data_source = is_data_source and component.find_last_result (split=split)
                else:
                    is_data_source = False
            self.cb.store_component_find_last_result_info (component, i)
        self.is_data_source[func][split] = is_data_source
        return is_data_source

    def find_last_fitted_model (self, split=None):
        self.load_all_estimators = False
        all_components_fitted = True
        for i, component in enumerate(self.components):
            self.cb.set_component_info (component, i)
            if isinstance (component, MultiComponent):
                if not component.find_last_fitted_model (split=split):
                    all_components_fitted = False
            elif (component.is_model and
                  not (component.data_io.can_load_model () and component.data_io.exists_estimator ())):
                    all_components_fitted = False
            self.cb.store_component_find_last_fitted_model_info (component, i)
        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted
        return all_components_fitted

# %% ../../nbs/core/compose.ipynb 101
class MultiModalityCallback (ParallelCallback):
    def __init__ (self, parallel, **kwargs):
        super ().__init__ (parallel, **kwargs)

    def select_input_to_fit (self, components, i, *X):
        X, y = self.data_converter.convert_varargs_to_x_y (X)
        if y is not None:
            return X[components[i].key], y
        else:
            return (X[components[i].key],)

    def initialize_result (self):
        return {component.key: None for component in self.components}

    def select_input (self, components, i, X):
        return X[components[i].key]

    def join_result (self, Xr, Xi_r, components, i):
        Xr[components[i].key] = Xi_r
        return Xr

class MultiModality (Parallel):
    """
    Analyzes multiple modalities using Parallel data flow.
    """
    def __init__ (self, *components, use_name=False, component_class=None, configs=None,
                    callback=None, **kwargs):
        """Assigns attributes and calls parent constructor.
        """
        if callback is None: callback = MultiModalityCallback (self, **kwargs)
        if component_class is not None and configs is not None and isinstance(configs, dict):
            new_components = []
            for k in configs:
                new_components.append (component_class (**configs[k], folder=k, suffix=k))
            components = list(components) + new_components
        super().__init__ (*components, callback=callback, **kwargs)
        for component in components:
            component.key = component.name if use_name else component.data_io.folder

    def __repr__ (self):
        return f'MultiModality {self.class_name} (name={self.name})'

# %% ../../nbs/core/compose.ipynb 107
class ColumnSelector (NoSaverComponent):
    def __init__ (self,
                  columns=[],
                  remainder=False,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, df, **kwargs):
        if self.remainder:
            return df[[c for c in df.columns if c not in self.columns]]
        else:
            return df[self.columns]

# %% ../../nbs/core/compose.ipynb 111
class Concat (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, *dfs, **kwargs):
        return pd.concat(list(dfs), axis=1)

# %% ../../nbs/core/compose.ipynb 115
class _BaseColumnTransformer (MultiComponent):
    def __init__ (self, name=None, class_name=None, direct_apply=True, direct_fit=True,**kwargs):
        super().__init__ (name=name, class_name=class_name, direct_apply=direct_apply,
                          direct_fit=direct_fit, **kwargs)
        self.concat = Concat (**kwargs)
        del self.concat.nick_name

    def set_components (self, *components):
        components = list(components)
        components.append (self.concat)
        super().set_components (*components)

    def _fit (self, df, y=None, **kwargs):
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        for component in self.components[:-1]:
            component.fit (df, **kwargs)
        return self

    def _apply (self, df, **kwargs):
        dfs = []
        assert len(self.components) > 0
        assert self.components[-1] is self.concat
        index = None
        for component in self.components[:-1]:
            df_component=component.transform (df, **kwargs)
            if df_component.shape[0]==df.shape[0]:
                if not (df_component.index==df.index).all():
                    message = f'index obtained by {component} is different than original index'
                    warnings.warn (message)
                    self.logger.warning (message)
                df_component.index=df.index
            elif index is not None:
                df_component.index=index
            else:
                index=df_component.index
            dfs.append (df_component)
        df_result = self.concat.transform (*dfs)
        return df_result

class ColumnTransformer (_BaseColumnTransformer):
    def __init__ (self, *transformers, remainder = 'drop', **kwargs):
        super().__init__ (**kwargs)
        components = make_column_transformer_pipelines (*transformers, remainder=remainder, **kwargs)
        super().set_components(*components)

# %% ../../nbs/core/compose.ipynb 117
class Identity (NoSaverComponent):
    def __init__ (self,
                  verbose=dflt.verbose,
                  force_verbose=False,
                  logger=None,
                  direct_apply=True,
                  **kwargs):
        verbose = 0 if not force_verbose else verbose
        if verbose==0:
            logger = set_empty_logger ()
        super().__init__ (verbose=verbose,
                          logger=logger,
                          direct_apply=direct_apply,
                          **kwargs)

    def _apply (self, X, **kwargs):
        return X

# %% ../../nbs/core/compose.ipynb 121
def _append_pipeline (pipelines, name, transformer, columns, remainder= False, **kwargs):
    drop = False
    if isinstance(transformer, str):
        if transformer == 'passthrough':
            transformer = Identity (**kwargs)
        elif transformer == 'drop':
            drop = True
        else:
            raise ValueError (f'name {transformer} not recognized')

    if not drop:
        config=kwargs.copy()
        config.update({name:dict(data_io='NoSaverIO')})
        config.update (direct_apply=True)
        pipeline = make_pipeline(ColumnSelector(columns, remainder=remainder, **kwargs),
                                 transformer,
                                 name=name,
                                 **config)
        pipelines.append (pipeline)

def _get_transformer_name (transformer, columns):
    columns_name = ''.join([x[0] for x in columns])
    if len(columns_name) > 5:
        columns_name = columns_name[:5]
    if isinstance(transformer,str):
        if transformer == 'passthrough':
            transformer_name = 'pass'
        elif transformer == 'drop':
            transformer_name = 'drop'
        else:
            raise ValueError (f'name {transformer} not recognized')
    elif hasattr(transformer, 'name'):
        transformer_name = transformer.name
    else:
        transformer_name = transformer.__class__.__name__
    name = f'{transformer_name}_{columns_name}'
    return name

def make_column_transformer_pipelines (*transformers, remainder='drop', **kwargs):
    pipelines = []
    all_columns = []
    for name, transformer, columns in transformers:
        _append_pipeline (pipelines, name, transformer, columns, **kwargs)
        all_columns.extend(columns)

    all_columns = list(set(all_columns))
    name = _get_transformer_name (remainder, ['r','e','m'])
    _append_pipeline (pipelines, name, remainder, all_columns, remainder=True, **kwargs)

    return pipelines

def make_column_transformer (*transformers, remainder='drop', name=None, class_name=None, **kwargs):
    """
    Given input tuples (`transformer`, `columns`), ..., builds a BaseColumnTransformer that
    applies `transformer` to `columns` of the input DataFrame. Similar to scikit-learn.
    `remainder` can be `passthrough`, `drop` (default), or an estimator / Component.
    """
    transformers_with_name = []
    for transformer, columns in transformers:
        transformer_name = _get_transformer_name (transformer, columns)
        transformers_with_name.append ((transformer_name, transformer, columns))

    pipelines = make_column_transformer_pipelines (*transformers_with_name,
                                                   remainder=remainder,
                                                   **kwargs)
    column_transformer = _BaseColumnTransformer (name=name, class_name=class_name, **kwargs)
    column_transformer.set_components(*pipelines)
    return column_transformer

# %% ../../nbs/core/compose.ipynb 138
class MultiSplitComponent (MultiComponent, metaclass=abc.ABCMeta):
    def __init__ (self,
                  component=None,
                  name=None,
                  class_name=None,
                  fit_to = 'training',
                  fit_additional = [],
                  apply_to = ['training', 'validation', 'test'],
                  raise_error_if_split_doesnot_exist=False,
                  raise_warning_if_split_doesnot_exist=True,
                  **kwargs):
        if class_name is None:
            if hasattr(component, 'class_name'):
                class_name = f'{component.class_name}MultiSplit'
            else:
                class_name = f'{component.__class__.__name__}MultiSplit'

        if name is None:
            if hasattr(component, 'name'):
                name = f'{component.name}_multi_split'
            else:
                name = f'{component.__class__.__name__}_multi_split'
        self.warning_if_nick_name_exists=False # needed when calling set_components in next line
        self.set_components (component)
        super().__init__ (name=name, class_name=class_name, **kwargs)

    @abc.abstractmethod
    def _initialize_fit (self, X):
        pass

    @abc.abstractmethod
    def _include_split_in_fit (self, additional_data, split, X):
        pass

    @abc.abstractmethod
    def _select_training_split (self, X):
        pass

    def _fit (self, X, y=None):
        X = self._initialize_fit (X)
        component = self.components[0]
        additional_data = {}
        for split in self.fit_additional:
            if split not in ['validation', 'test']:
                raise ValueError (f'split {split} not valid')
            self._include_split_in_fit (additional_data, split, X)
        X_fit = self._select_training_split (X)
        component.fit(X_fit, y=y, split=self.fit_to, **additional_data)

    @abc.abstractmethod
    def _get_split_keys (self, X):
        pass

    def _issue_error_or_warning (self, split, X):
        message = f'split {split} not found in X ({self._get_split_keys (X)})'
        if self.raise_error_if_split_doesnot_exist:
            raise RuntimeError (message)
        elif self.raise_warning_if_split_doesnot_exist:
            warnings.warn (message)

    @abc.abstractmethod
    def _initialize_apply (self, X, apply_to, split):
        pass

    @abc.abstractmethod
    def _included_split (self, split, X):
        pass

    @abc.abstractmethod
    def _select_split (self, X, split):
        pass

    @abc.abstractmethod
    def _convert_result (self, result, input_not_dict, output_not_dict):
        pass

    @abc.abstractmethod
    def _initialize_result (self):
        pass

    @abc.abstractmethod
    def _add_result (self, result, split, result_split, X):
        pass

    def _apply (self, X, apply_to = None, output_not_dict=False, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]
        X, input_not_dict = self._initialize_apply (X, apply_to, split)

        component = self.components[0]
        result = self._initialize_result ()

        for split in apply_to:
            if self._included_split (split, X):
                X_split = self._select_split (X, split)
                result_split = component.apply (X_split, split=split, **kwargs)
                result = self._add_result (result, split, result_split, X)
            else:
                self._issue_error_or_warning (split, X)

        result = self._convert_result (result, input_not_dict, output_not_dict)
        return result

    def find_last_result (self, apply_to = None, split=None, **kwargs):
        apply_to = self.apply_to if apply_to is None else apply_to
        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]

        self.is_data_source = True
        component = self.component
        for split in apply_to:
            if not (component.data_io.can_load_result () and component.data_io.exists_result (split=split)):
                if isinstance (component, MultiComponent):
                    # TODO: have one flag is_data_source per split
                    # or make MultiSplitComponent a Parallel object
                    # or always use DataFrame
                    self.is_data_source = self.is_data_source and component.find_last_result (split=split)
                else:
                    self.is_data_source = False
        return self.is_data_source

    def find_last_fitted_model (self, apply_to = None, split=None, **kwargs):
        all_components_fitted = True
        self.load_all_estimators = False
        split = self.fit_to
        component = self.component
        if isinstance (component, MultiComponent):
            if not component.find_last_fitted_model (split=split):
                all_components_fitted = False
        elif (component.is_model and
              not (component.data_io.can_load_model () and component.data_io.exists_estimator ())):
                all_components_fitted = False

        if all_components_fitted and self.data_io.exists_result (split=split):
            self.data_io.load_estimator = self.data_io.load_estimators
            self.load_all_estimators = True
        self.all_components_fitted = all_components_fitted

        return all_components_fitted

# %% ../../nbs/core/compose.ipynb 140
class MultiSplitDict (MultiSplitComponent):
    def __init__ (self, component=None, **kwargs):
        super().__init__ (component=component, **kwargs)

    def __repr__ (self):
        return f'MultiSplitDict {self.class_name} (name={self.name})'

    def _initialize_fit (self, X):
        if not isinstance(X, dict):
            X = {self.fit_to: X}
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.keys():
            additional_data[f'{split}_data'] = X[split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X):
        return X[self.fit_to]

    def _get_split_keys (self, X):
        return X.keys()

    def _initialize_apply (self, X, apply_to, split):
        if not isinstance(X, dict):
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X = {key: X}
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        return split in X.keys()

    def _select_split (self, X, split):
        return X[split]

    def _initialize_result (self):
        return {}

    def _add_result (self, result, split, result_split, X):
        result[split] = result_split
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        if input_not_dict:
            result = result[self.key]
        elif output_not_dict and len(result)==1:
            result = list(result.items())[0][1]
        return result

# %% ../../nbs/core/compose.ipynb 161
class MultiSplitDFColumn (MultiSplitComponent):
    def __init__ (self, component=None, apply_to=['whole'], drop_split=False, add_split=True, **kwargs):
        if (getattr(component, 'apply_to_separate_splits', None)==True
            and ((isinstance (apply_to, list) and apply_to==['whole'])
                 or (isinstance(apply_to, str) and apply_to=='whole'))):
            apply_to = ['training', 'validation', 'test']
        super().__init__ (component=component, apply_to=apply_to, **kwargs)

    def __repr__ (self):
        return f'MultiSplitDF {self.class_name} (name={self.name})'

    def _initialize_fit (self, X):
        if 'split' not in X.columns:
            # X.index = pd.MultiIndex.from_arrays ([[self.fit_to]*X.shape[0], X.index], names=('split', 'number'))
            X['split'] = self.fit_to
        return X

    def _include_split_in_fit (self, additional_data, split, X):
        if split in X.split.values:
            additional_data[f'{split}_data'] = X[X.split==split]
        else:
            self._issue_error_or_warning (split, X)

    def _select_training_split (self, X):
        return X[X.split==self.fit_to]

    def _get_split_keys (self, X):
        #X.index.get_level_values(0).unique()
        return X.split.unique()

    def _initialize_apply (self, X, apply_to, split):
        if 'split' not in X.columns:
            key = apply_to[0] if len(apply_to)==1 else split if split is not None else 'test'
            X['split'] = key
            input_not_dict = True
            self.key = key
        else:
            input_not_dict = False

        return X, input_not_dict

    def _included_split (self, split, X):
        #split in X.index.get_level_values(0).values
        return split == 'whole' or split in X.split.values

    def _select_split (self, X, split):
        X_split = X[X.split==split] if split != 'whole' else X
        if self.drop_split: X_split = X_split.drop(columns='split')
        return X_split

    def _initialize_result (self):
        return []

    def _add_result (self, result, split, result_split, X):
        if self.drop_split or (not hasattr(result_split, 'split') and self.add_split):
            if split=='whole': result_split['split'] = X.split
            else: result_split['split']=split
        result.append (result_split)
        return result

    def _convert_result (self, result, input_not_dict, output_not_dict):
        result = pd.concat (result, axis=0)
        return result

# %% ../../nbs/core/compose.ipynb 173
class ParallelInstancesCallback (ParallelCallback):
    def __init__ (self, parallel, **kwargs):
        self.storage = None
        super ().__init__ (parallel, **kwargs)

    def create_component_storage_info (self):
        self.storage = Bunch (start_idx=[self.start_idx]*self.n_iterations,
                              is_data_source=[self.is_data_source]*self.n_iterations,
                              all_components_fitted=[self.all_components_fitted]*self.n_iterations,
                              load_all_estimators=[self.load_all_estimators]*self.n_iterations)

    def set_component_config (self, component, i):
        suffix = self.configs[i].get('suffix', '')
        component.set_suffix (suffix)

    def set_component_info (self, component, i):
        if self.storage is None: self.create_component_storage_info ()
        self.set_component_config (component, i)
        component.start_idx = copy.deepcopy(self.storage.start_idx[i])
        component.is_data_source = copy.deepcopy(self.storage.is_data_source[i])
        component.all_components_fitted = self.storage.all_components_fitted[i]
        component.load_all_estimators = self.storage.load_all_estimators[i]

    def store_component_find_last_result_info (self, component, i):
        self.storage.start_idx[i] = copy.deepcopy(component.start_idx)
        self.storage.is_data_source[i] = copy.deepcopy(component.is_data_source)

    def store_component_find_last_fitted_model_info (self, component, i):
        self.storage.start_idx[i] = copy.deepcopy(component.start_idx)
        self.storage.is_data_source[i] = copy.deepcopy(component.is_data_source)
        self.storage.all_components_fitted[i] = component.all_components_fitted
        self.storage.load_all_estimators[i] = component.load_all_estimators

class ParallelInstances (Parallel):
    """
    Runs the same instance in a parallel loop.

    The main difference with parallal, is that information specific about each iteration of the loop
    is held in the ParallelInstances object, instead of the component object. Examples of this are:
        - start_idx, is_data_source, all_components_fitted, load_all_estimators
        - In ParallelCV, the train / test indexes to use for cross-validation.
        - The suffix to be used for each component.
        - Other things.
    """
    def __init__ (self, component, configs=[], n_iterations=None, callback=None, **kwargs):
        """Assigns attributes and calls parent constructor.
        """
        if callback is None: callback = ParallelInstancesCallback (self, **kwargs)
        n_iterations = len(configs) if n_iterations is None else n_iterations
        components = (component, ) * n_iterations
        super().__init__ (*components, callback=callback, **kwargs)
        self.cb.create_component_storage_info ()

    def __repr__ (self):
        return f'ParallelInstances {self.class_name} (name={self.name})'

    def set_unique_names (self):
        self.register_global_name (self)
        component = self.components[0]
        if isinstance (component, MultiComponent):
            component.set_unique_names ()
        else:
            self.register_global_name (component)

# %% ../../nbs/core/compose.ipynb 175
class CrossValidatorCallback (ParallelInstancesCallback):
    def __init__ (self, parallel, **kwargs):
        super ().__init__ (parallel, **kwargs)
        self.dict_results = None
        self.stored_fit_info = False

    def store_component_fit_info (self, component, i):
        if self.score_method is not None:
            score_method = self.find_method (self.score_method)
            if score_method is None:
                raise ValueError (f'score method {self.score_method} not found in {component}')

            dict_results = score_method()
            self._add_dict_results (dict_results)
            if self.trial is not None:
                if self.key_score is None:
                    self.logger.warning (f'key_score is None but trial is {self.trial}')
                else:
                    self.parallel._apply_pruner (dict_results, i)

    store_component_fit_apply_info = store_component_fit_info

    def _add_dict_results (self, dict_results):
        dict_results = copy.deepcopy(dict_results)
        if self.dict_results is None:
            self.dict_results = {k: np.array(v) if isinstance(v, list) else v
                                 for k, v in dict_results.items()}
        else:
            for k in dict_results: self.dict_results[k] += dict_results[k]

    def join_result (self, Xr, Xi_r, components, i):
        if self.evaluator is not None and self.add_evaluation:
            self._add_dict_results (Xi_r)
            return None
        elif self.dict_results is None:
            return super().join_result (Xr, Xi_r, components, i)

    def store_fit_info (self):
        if self.stored_fit_info:
            return
        if self.dict_results is not None:
            for k in self.dict_results:
                self.dict_results[k] = self.dict_results[k] / self.n_iterations
        self.data_io.save_result (self.dict_results, result_file_name='cross_validation_metrics.pk')
        if self.select_epoch:
            final_dict_results = self.dict_results.copy()
            for k in self.dict_results:
                if isinstance (self.dict_results[k], np.ndarray):
                    final_dict_results[f'last_{k}'] = self.dict_results[k][-1]
                    final_dict_results['n_iterations'] = self.n_iterations
                    if self.optimization_mode is None or self.optimization_mode=='max':
                        final_dict_results[f'argmax_{k}'] = np.argmax(self.dict_results[k])
                        final_dict_results[f'{self.max_prefix}{k}'] = np.max(self.dict_results[k])
                    if self.optimization_mode is None or self.optimization_mode=='min':
                        final_dict_results[f'argmin_{k}'] = np.argmin(self.dict_results[k])
                        final_dict_results[f'{self.min_prefix}{k}'] = np.min(self.dict_results[k])
                    if self.optimization_mode is None: del final_dict_results[k]
            self.dict_results = final_dict_results
            self.parallel.dict_results = self.dict_results
        self.stored_fit_info = True
        self.data_io.save_result (self.dict_results, result_file_name='cross_validation_final_metrics.pk')

    def finalize_result (self, Xr, components=None):
        if self.dict_results is not None:
            self.cb.store_fit_info ()
            result = self.dict_results
        else:
            result = super().finalize_result (Xr, components=components)
        self.parallel.dict_results = self.dict_results
        return result

class CrossValidator (ParallelInstances):
    """
    Runs cross-validation on given pipeline.
    """
    def __init__ (self, component, splitter=None, evaluator=None, n_iterations=None, score_method=None,
                  select_epoch=False, add_evaluation=True, optimization_mode=None, trial=None,
                  key_score=None, pruner_optimization_mode=None, indicate_same_step=False,
                  callback=None, **kwargs):
        """Assigns attributes and calls parent constructor."""
        if callback is None: callback = CrossValidatorCallback (self, **kwargs)
        components = (splitter, component) if splitter is not None else (component, )
        components += (evaluator, ) if evaluator is not None else ()
        pipeline = Sequential (*components, **kwargs) if len(components)>1 else component

        assert splitter is not None or n_iterations is not None, 'either splitter or n_iterations need to be specified'
        n_iterations = splitter.split_generator.get_n_splits() if n_iterations is None else n_iterations
        configs = [dict(suffix=i) for i in range(n_iterations)]

        super().__init__ (pipeline, configs=configs, n_iterations=n_iterations, callback=callback, **kwargs)
        if optimization_mode is None:
            self.max_prefix = 'max_'
            self.min_prefix = 'min_'
        else:
            self.max_prefix = ''
            self.min_prefix = ''
        if self.trial is not None and self.key_score is not None and self.pruner_optimization_mode is None:
            self.pruner_optimization_mode = self.optimization_mode

    def _apply_pruner (self, dict_results, i):
        result = dict_results[self.key_score]
        if self.pruner_optimization_mode=='max': result = np.max(result)
        elif self.pruner_optimization_mode=='min': result = np.min(result)
        step_number = 0 if self.indicate_same_step else i
        self.logger.debug (f'reporting {result} at step {step_number}')
        self.trial.report (result, step_number)
        if self.trial.should_prune():
            self.logger.info (f'prunning at {i}-th fold')
            self.force_end = True
            self.n_iterations = i+1

# %% ../../nbs/core/compose.ipynb 207
def finalize_result_parallel_models (self, Xr, components=None):
    Xr = np.array(Xr)
    assert Xr.shape[0] == self.n_models or Xr.shape[1] == self.n_models
    if Xr.shape[1]==self.n_models: Xr = Xr.T
    return Xr

# %% ../../nbs/core/compose.ipynb 209
class ParallelModelInstancesCallback (ParallelInstancesCallback):
    def __init__ (self, parallel, **kwargs):
        super ().__init__ (parallel, **kwargs)

    def set_component_config (self, component, i):
        super().set_component_config (component, i) # this needs to be called first
        if self.separate_model_paths is not None:
            component.data_io.set_full_path_models (self.separate_model_paths[i])

    def finalize_result (self, Xr, components=None):
        return finalize_result_parallel_models (self, Xr, components=components)

class ParallelModelInstances (ParallelInstances):
    """
    Ensemble of given instances
    """
    def __init__ (self, component, separate_model_paths=None, n_models=None, callback=None, **kwargs):
        """Assigns attributes and calls parent constructor."""
        if callback is None: callback = ParallelModelInstancesCallback (self, **kwargs)
        if n_models is None:
            if separate_model_paths is not None:
                n_models = len(separate_model_paths)
            else:
                raise ValueError ('either n_models or separate_model_paths need to be indicated')
        elif separate_model_paths is not None:
            assert n_models==len(separate_model_paths)

        configs = [dict(suffix=i) for i in range(n_models)]

        super().__init__ (component, configs=configs, n_iterations=n_models, callback=callback, **kwargs)

# %% ../../nbs/core/compose.ipynb 211
class WeightedClassifier (Component):
    def __init__ (self, weights=None, **kwargs):
        super().__init__ (**kwargs)
    def _apply (self, X, **kwargs):
        if self.weights is None:
            n_models = X.shape[0]
            self.weights = np.ones ((n_models, )) * 1.0/n_models
        return (self.weights.reshape(-1,1) * X).mean(axis=0)

# %% ../../nbs/core/compose.ipynb 213
def make_ensembler (component, instances=True, final_classifier=None, root=False, class_name='Ensembler',
                    name='ensembler', **kwargs):
    """Assigns attributes and calls parent constructor."""
    if instances:
        parallel_models = ParallelModelInstances (component, **kwargs)
    else:
        raise NotImplementedError ('Only ParallelModelInstances is implemented as of now.')
    if final_classifier is None:
        final_classifier = WeightedClassifier (**kwargs)

    return Sequential (parallel_models, final_classifier, root=root, class_name=class_name, name=name,
                       **kwargs)
